{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Setup ----------------------------\n",
    "PATH_GENERATED = './generated/'\n",
    "\n",
    "vocab = torch.load(PATH_GENERATED + 'vocabulary.pt', map_location=torch.device(device))\n",
    "embedding = torch.load(PATH_GENERATED + 'embedding.pt', map_location=torch.device(device))\n",
    "(VOCAB_SIZE, embedding_dim) = embedding.weight.shape  \n",
    "\n",
    "words_train = torch.load(PATH_GENERATED + \"words_train.pt\", map_location=torch.device(device))\n",
    "words_val   = torch.load(PATH_GENERATED + \"words_val.pt\", map_location=torch.device(device))\n",
    "words_test  = torch.load(PATH_GENERATED + \"words_test.pt\", map_location=torch.device(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 10\n",
    "conjugation_list = ['be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "MAP_TARGET = {vocab[w]:i for i, w in enumerate(conjugation_list)}\n",
    "\n",
    "def create_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=MAP_TARGET):\n",
    "    \n",
    "    # Transform each word to its index in the vocabulary.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        t = txt[i + context_size]\n",
    "        if  not vocab.lookup_token(t) in conjugation_list: continue # We only want to guess conjunctions of be and have.\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(map_target[t]) \n",
    "        contexts.append(torch.tensor(c).to(device=device))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets).to(device=device)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname, map_location=torch.device(device))\n",
    "    else:\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train_conj = load_dataset(words_train, vocab, \"conj_data_train.pt\")\n",
    "data_val_conj   = load_dataset(words_val, vocab, \"conj_data_val.pt\")\n",
    "data_test_conj  = load_dataset(words_test, vocab, \"conj_data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "counts = Counter(data_train_conj[:][1].tolist())\n",
    "values = [int(counts[i]) for i in range(len(counts))]\n",
    "plt.bar(conjugation_list, values)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the target words are very skewed, we need to weigh the words by their frequency in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calculate_word_weights(values):\n",
    "    \"\"\"\n",
    "    Calculate the weight of each word so that the loss function can weigh \n",
    "    frequent words less and unfrequent words more.\n",
    "    \"\"\"\n",
    "    total_words = sum(values)\n",
    "    word_weights = [total_words / value for value in values]\n",
    "    word_weights = torch.tensor(word_weights, dtype=torch.float).to(device=device)\n",
    "    return word_weights\n",
    "\n",
    "word_weigts = calculate_word_weights(values)\n",
    "values_balanced = [int(values[i])*w for i, w in enumerate(word_weigts)]\n",
    "plt.bar(conjugation_list, values_balanced)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Attention functions -----------------------\n",
    "pos_encoding = torch.zeros((CONTEXT_SIZE, embedding_dim))\n",
    "for i in range(CONTEXT_SIZE):\n",
    "    for ii in range(int(embedding_dim/2)):\n",
    "        pos_encoding[i][2*ii] = math.sin(i/1000**((2*ii)/embedding_dim))\n",
    "        pos_encoding[i][2*ii +1] = math.cos(i/1000**((2*ii)/embedding_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Simple MLP hyper parameters -----------------------\n",
    "lrs = [0.01, 0.001]\n",
    "decays = [0.8, 0.5, 0.1]\n",
    "mlp_hparams = [{\n",
    "    'lr': lr,\n",
    "    'weight_decay': decay\n",
    "} for lr in lrs for decay in decays]\n",
    "\n",
    "# ---------------- Attention MLP hyper parameters ----------------------- TEMP\n",
    "lrs = [0.1, 0.01, 0.001]\n",
    "attention_hparams = [{\n",
    "    'lr': lr,\n",
    "} for lr in lrs]\n",
    "\n",
    "# ---------------- RNN MLP hyper parameters ----------------------- TEMP\n",
    "lrs = [0.01, 0.001]\n",
    "rnn_hparams = [{\n",
    "    'lr': lr,\n",
    "} for lr in lrs]\n",
    "\n",
    "hparams = []\n",
    "hparams.append(mlp_hparams)\n",
    "hparams.append([None]) #TODO temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Train and accuracy -----------------------\n",
    "def train(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "def compute_accuracy(model, loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy as correctly predicted / total \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += len(targets)\n",
    "            correct += int((predicted == targets).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, embedding, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "\n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class AttentionMLP(nn.Module):\n",
    "    def __init__(self, batch_size=256, n_heads=5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.embedding = nn.Embedding(VOCAB_SIZE, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.W_q = nn.Parameter(torch.rand((self.n_heads, CONTEXT_SIZE, embedding_dim)))\n",
    "        self.W_k = nn.Parameter(torch.rand((self.n_heads, CONTEXT_SIZE, embedding_dim)))\n",
    "        self.W_v = nn.Parameter(torch.rand((self.n_heads, CONTEXT_SIZE, embedding_dim)))\n",
    "        self.W_o = nn.Parameter(torch.rand((embedding_dim, self.n_heads * CONTEXT_SIZE)))\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*CONTEXT_SIZE, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, _ = x.size()\n",
    "        x = self.embedding(x)\n",
    "        x = x + pos_encoding\n",
    "        \n",
    "        multi_attn_heads = []\n",
    "        for ii in range(CONTEXT_SIZE):\n",
    "            attn_heads = torch.empty(0)\n",
    "            for i in range(self.n_heads):\n",
    "                q = x[:,ii,:] @ self.W_q[i].T\n",
    "                k = x[:,ii,:] @ self.W_k[i].T \n",
    "                v = x[:,ii,:] @ self.W_v[i].T\n",
    "                \n",
    "                attn_weights = torch.stack([q[j] @ k[j].T for j in range(batch_size) ])\n",
    "                attn_weights = torch.softmax(attn_weights / math.sqrt(CONTEXT_SIZE), dim=-1)\n",
    "                attn_head = torch.unsqueeze(attn_weights, dim=1) * v\n",
    "                attn_heads = torch.cat((attn_heads, attn_head), 1)\n",
    "            multi_attn_heads.append(self.W_o @ attn_heads.T)\n",
    "       \n",
    "        multi_attn_heads = torch.stack(multi_attn_heads)\n",
    "        out = multi_attn_heads.permute(2, 0, 1)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training a simple MLP model\n",
      "Training using parameters: {'lr': 0.01, 'weight_decay': 0.8}\n",
      "16:17:48.253302  |  Epoch 1  |  Training loss 2.48206\n",
      "16:17:48.598952  |  Epoch 5  |  Training loss 2.48074\n",
      "16:17:49.010302  |  Epoch 10  |  Training loss 2.48088\n",
      "\n",
      "Training using parameters: {'lr': 0.01, 'weight_decay': 0.5}\n",
      "16:17:49.113969  |  Epoch 1  |  Training loss 2.48095\n",
      "16:17:49.454666  |  Epoch 5  |  Training loss 2.47925\n",
      "16:17:49.837388  |  Epoch 10  |  Training loss 2.47889\n",
      "\n",
      "Training using parameters: {'lr': 0.01, 'weight_decay': 0.1}\n",
      "16:17:49.930799  |  Epoch 1  |  Training loss 2.47463\n",
      "16:17:50.252930  |  Epoch 5  |  Training loss 2.47040\n",
      "16:17:50.657184  |  Epoch 10  |  Training loss 2.46892\n",
      "\n",
      "Training using parameters: {'lr': 0.001, 'weight_decay': 0.8}\n",
      "16:17:50.758534  |  Epoch 1  |  Training loss 2.48178\n",
      "16:17:51.115362  |  Epoch 5  |  Training loss 2.48106\n",
      "16:17:51.484919  |  Epoch 10  |  Training loss 2.48069\n",
      "\n",
      "Training using parameters: {'lr': 0.001, 'weight_decay': 0.5}\n",
      "16:17:51.573263  |  Epoch 1  |  Training loss 2.49068\n",
      "16:17:51.873095  |  Epoch 5  |  Training loss 2.48102\n",
      "16:17:52.225988  |  Epoch 10  |  Training loss 2.47930\n",
      "\n",
      "Training using parameters: {'lr': 0.001, 'weight_decay': 0.1}\n",
      "16:17:52.317518  |  Epoch 1  |  Training loss 2.47916\n",
      "16:17:52.611020  |  Epoch 5  |  Training loss 2.47588\n",
      "16:17:52.981681  |  Epoch 10  |  Training loss 2.47105\n",
      "\n",
      "Now training a MLP model with attention\n",
      "Training using parameters: {'lr': 0.1}\n",
      "16:18:01.104901  |  Epoch 1  |  Training loss 7.17947\n",
      "16:18:32.258074  |  Epoch 5  |  Training loss 2.46711\n",
      "16:19:11.163080  |  Epoch 10  |  Training loss 2.46877\n",
      "\n",
      "Training using parameters: {'lr': 0.01}\n",
      "16:19:19.868579  |  Epoch 1  |  Training loss 2.77792\n",
      "16:19:51.176737  |  Epoch 5  |  Training loss 2.45509\n",
      "16:20:30.154729  |  Epoch 10  |  Training loss 2.44868\n",
      "\n",
      "Training using parameters: {'lr': 0.001}\n",
      "16:20:38.660830  |  Epoch 1  |  Training loss 2.66977\n",
      "16:21:09.942072  |  Epoch 5  |  Training loss 2.44800\n",
      "16:21:49.171120  |  Epoch 10  |  Training loss 2.44410\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_models(hparams):\n",
    "    mlp_hparams, _ = hparams\n",
    "    \n",
    "    train_losses = []\n",
    "    models = []\n",
    "    accuracies = []\n",
    "\n",
    "    print(\"Now training a simple MLP model\")\n",
    "    for param in mlp_hparams:\n",
    "        print(f\"Training using parameters: {param}\")\n",
    "        model = SimpleMLP(embedding)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "        optimizer = optim.Adam(model.parameters(), **param)\n",
    "        train_loader = DataLoader(data_test_conj, batch_size=128, shuffle=True)\n",
    "        val_loader = DataLoader(data_val_conj, batch_size=128, shuffle=True)\n",
    "        n_epochs = 10\n",
    "        \n",
    "        loss = train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "        accuracy = compute_accuracy(model, val_loader)\n",
    "\n",
    "        models.append(model)\n",
    "        train_losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        print()\n",
    "\n",
    "    print(\"Now training a MLP model with attention\")\n",
    "    for param in attention_hparams:\n",
    "        print(f\"Training using parameters: {param}\")\n",
    "        model = AttentionMLP(embedding)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "        optimizer = optim.Adam(model.parameters(), **param)\n",
    "        train_loader = DataLoader(data_test_conj, batch_size=128, shuffle=True)\n",
    "        val_loader = DataLoader(data_val_conj, batch_size=128, shuffle=True)\n",
    "        n_epochs = 10\n",
    "        \n",
    "        loss = train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "        accuracy = compute_accuracy(model, val_loader)\n",
    "\n",
    "        models.append(model)\n",
    "        train_losses.append(loss)\n",
    "        accuracies.append(accuracy)\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    return models, train_losses, accuracies\n",
    "\n",
    "models, train_losses, accuracies = train_models(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4]) torch.Size([2, 3, 4]) torch.Size([2, 3, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "# Define the input tensors\n",
    "x = torch.rand((2,3,4))\n",
    "y = torch.rand((2,3,4))\n",
    "\n",
    "z = [x, y]\n",
    "z = torch.stack(z, dim=2)\n",
    "print(x.size(), y.size(), z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9160, 0.8294, 0.4930, 0.2672],\n",
       "          [0.9036, 0.4186, 0.5404, 0.7261],\n",
       "          [0.3454, 0.4073, 0.9920, 0.2930]],\n",
       "\n",
       "         [[0.8204, 0.1322, 0.0439, 0.7257],\n",
       "          [0.9570, 0.4470, 0.4745, 0.9710],\n",
       "          [0.1387, 0.8025, 0.5025, 0.8648]]],\n",
       "\n",
       "\n",
       "        [[[0.6423, 0.4596, 0.4643, 0.3905],\n",
       "          [0.7047, 0.6351, 0.6018, 0.8730],\n",
       "          [0.8952, 0.6704, 0.9411, 0.8374]],\n",
       "\n",
       "         [[0.6498, 0.3961, 0.1706, 0.3020],\n",
       "          [0.5616, 0.4951, 0.1619, 0.4597],\n",
       "          [0.1409, 0.8088, 0.7118, 0.0246]]]])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inf265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
