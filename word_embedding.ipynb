{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from datetime import datetime\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "PATH_GENERATED = './generated/'\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith('.txt')]\n",
    "\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            texts += f.readlines()\n",
    "    return texts\n",
    "\n",
    "def tokenize(texts, tokenizer=TOKENIZER):\n",
    "    tokenized_text = []\n",
    "    for text in texts:\n",
    "        tokenized_text += tokenizer(text)\n",
    "    return tokenized_text\n",
    "\n",
    "def yield_tokens(texts, tokenizer=TOKENIZER):\n",
    "    \"\"\"\n",
    "    Remove yield tokens from the text before tokenizing\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove words with digits, upper case, and multiple space \n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    no_spaces = '\\s+'\n",
    "\n",
    "    for text in texts:\n",
    "        text = re.sub(no_digits, ' ', text)\n",
    "        text = re.sub(no_names, ' ', text)\n",
    "        text = re.sub(no_spaces, ' ', text)\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    vocab.append_token(\"i\")  # Upper case words like 'I' were removed so we should add it back again.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def calculate_word_weights(freqs):\n",
    "    \"\"\"\n",
    "    Calculate the weight of each word so that the loss function can weigh \n",
    "    frequent words less and unfrequent words more.\n",
    "    \"\"\"\n",
    "    total_words = sum(freqs)\n",
    "    word_weights = [total_words / (len(freqs)* freq) for freq in freqs]\n",
    "    word_weights = torch.tensor(word_weights, dtype=torch.float)\n",
    "    return word_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Tokenize texts -------------------------------\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\")\n",
    "    words_val   = torch.load(PATH_GENERATED + \"words_val.pt\")\n",
    "    words_test  = torch.load(PATH_GENERATED + \"words_test.pt\")\n",
    "else:\n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val   = read_files('./data_val/')\n",
    "    lines_books_test  = read_files('./data_test/')\n",
    "\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val   = tokenize(lines_books_val)\n",
    "    words_test  = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train, PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val, PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test, PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME)\n",
    "else:\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "occurences = [(f.item(), w) for (f, w) in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))]\n",
    "word_weigts = calculate_word_weights(freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      2684706\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   52105\n",
      "Number of distinct words kept (vocabulary size):    1880\n",
      "The 10 most occuring words:\n",
      " [(433907, '<unk>'), (182537, ','), (151278, 'the'), (123727, '.'), (82289, 'and'), (65661, 'of'), (62763, 'to'), (49230, 'a'), (41477, 'in'), (31052, 'that')]\n"
     ]
    }
   ],
   "source": [
    "n_print = 10\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "print(f\"The {n_print} most occuring words:\\n {occurences[:n_print]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))}\n",
    "\n",
    "def create_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=MAP_TARGET):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform each word to its index in the vocabulary.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        t = txt[i + context_size]\n",
    "        if map_target[t] < 2: continue # We only want to guess actual words.\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(t) \n",
    "        contexts.append(torch.tensor(c).to(device=device))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets).to(device=device)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname)\n",
    "    else:\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val   = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test  = load_dataset(words_test, vocab, \"data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))        \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader, device=None):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += len(targets)\n",
    "            correct += int((predicted == targets).sum())\n",
    "\n",
    "    acc =  correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are testing 6 different hyper parameters.\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "embedding_dims = [10, 16]\n",
    "\n",
    "hparams = [{\n",
    "    'batch_size': bs,\n",
    "    'embedding_dim': em\n",
    " } for bs in batch_sizes for em in embedding_dims]\n",
    "\n",
    "print(f\"We are testing {len(hparams)} different hyper parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model():\n",
    "\n",
    "    # ---------------- Train many models ----------------------- \n",
    "    models = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for param in hparams:\n",
    "        print(f'Now training with parameters {param}')\n",
    "        train_loader = DataLoader(data_train, batch_size=param['batch_size'], shuffle=True)\n",
    "        val_loader   = DataLoader(data_val, batch_size=param['batch_size'], shuffle=True)\n",
    "\n",
    "        embedding = nn.Embedding(VOCAB_SIZE, param['embedding_dim'])\n",
    "        torch.manual_seed(seed)\n",
    "        model = Word2Vec(embedding).to(device=device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "        n_epochs=5\n",
    "        train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "\n",
    "        train_acc = compute_accuracy(model, train_loader)\n",
    "        print(f'Train accuracy: {train_acc}')\n",
    "        val_acc = compute_accuracy(model, val_loader)\n",
    "        print(f'Val accuracy:   {val_acc}')\n",
    "        \n",
    "        models.append(model)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        print()\n",
    "\n",
    "    # ---------------- Retrain the best performing model for longer on more data -----------------------   \n",
    "    best_idx = val_accs.index(max(val_accs))\n",
    "    best_model = models[best_idx]\n",
    "    best_param = hparams[best_idx]\n",
    "    \n",
    "    print(f'The best model had these parameters: {best_param}.')\n",
    "    train_val_loader = DataLoader(ConcatDataset([data_train, data_val]), batch_size=best_param['batch_size'], shuffle=True)\n",
    "    embedding = nn.Embedding(VOCAB_SIZE, best_param['embedding_dim'])\n",
    "    torch.manual_seed(seed)\n",
    "    best_model = Word2Vec(embedding).to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "    n_epochs=50\n",
    "    losses_train = train(n_epochs, optimizer, best_model, loss_fn, train_val_loader)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now training with parameters {'batch_size': 64, 'embedding_dim': 10}\n",
      "18:30:58.601184  |  Epoch 1  |  Training loss 4.21664\n",
      "18:38:10.999905  |  Epoch 5  |  Training loss 4.05266\n",
      "Train accuracy: 0.22833140202100569\n",
      "Val accuracy:   0.21878723017587787\n",
      "\n",
      "Now training with parameters {'batch_size': 64, 'embedding_dim': 16}\n",
      "18:41:32.108769  |  Epoch 1  |  Training loss 4.16823\n",
      "18:45:10.835556  |  Epoch 5  |  Training loss 4.02985\n",
      "Train accuracy: 0.2330201888253561\n",
      "Val accuracy:   0.22068533812571936\n",
      "\n",
      "Now training with parameters {'batch_size': 128, 'embedding_dim': 10}\n",
      "18:46:03.960646  |  Epoch 1  |  Training loss 4.23304\n",
      "18:47:58.293229  |  Epoch 5  |  Training loss 3.91994\n",
      "Train accuracy: 0.2371517445318905\n",
      "Val accuracy:   0.2269450558326434\n",
      "\n",
      "Now training with parameters {'batch_size': 128, 'embedding_dim': 16}\n",
      "18:48:45.753994  |  Epoch 1  |  Training loss 4.17842\n",
      "18:50:58.741807  |  Epoch 5  |  Training loss 3.89441\n",
      "Train accuracy: 0.2404120679270668\n",
      "Val accuracy:   0.22738929386345738\n",
      "\n",
      "Now training with parameters {'batch_size': 256, 'embedding_dim': 10}\n",
      "18:51:36.623600  |  Epoch 1  |  Training loss 4.28444\n",
      "18:53:04.718469  |  Epoch 5  |  Training loss 3.90387\n",
      "Train accuracy: 0.23823342842765102\n",
      "Val accuracy:   0.22882297114472064\n",
      "\n",
      "Now training with parameters {'batch_size': 256, 'embedding_dim': 16}\n",
      "18:53:41.476178  |  Epoch 1  |  Training loss 4.22294\n",
      "18:55:09.596461  |  Epoch 5  |  Training loss 3.86714\n",
      "Train accuracy: 0.24242420856236238\n",
      "Val accuracy:   0.22944894291541304\n",
      "\n",
      "The best model had these parameters: {'batch_size': 256, 'embedding_dim': 16}.\n",
      "18:55:46.824955  |  Epoch 1  |  Training loss 4.22540\n",
      "18:57:16.790851  |  Epoch 5  |  Training loss 3.86460\n",
      "18:59:09.170640  |  Epoch 10  |  Training loss 3.81581\n",
      "19:01:01.608196  |  Epoch 15  |  Training loss 3.79831\n",
      "19:02:51.046059  |  Epoch 20  |  Training loss 3.79019\n",
      "19:04:39.805350  |  Epoch 25  |  Training loss 3.78609\n",
      "19:06:27.430033  |  Epoch 30  |  Training loss 3.78461\n",
      "19:08:14.277906  |  Epoch 35  |  Training loss 3.78458\n",
      "19:10:01.158913  |  Epoch 40  |  Training loss 3.78544\n",
      "19:11:48.008722  |  Epoch 45  |  Training loss 3.78682\n",
      "19:13:34.944646  |  Epoch 50  |  Training loss 3.78877\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Best Model -------------------------------\n",
    "if os.path.isfile(PATH_GENERATED + 'best_model.pt'):\n",
    "    best_model = torch.load(PATH_GENERATED + 'best_model.pt')\n",
    "else:\n",
    "    best_model = train_best_model()\n",
    "    torch.save(best_model, PATH_GENERATED + 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineSimilarity = nn.CosineSimilarity()\n",
    "embedding_matrix = best_model.embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_matrix = []\n",
    "for i in range(len(embedding_matrix)):\n",
    "    for ii in range(len(embedding_matrix)):    \n",
    "        cosine_sim_matrix.append(cosineSimilarity(embedding_matrix[i].unsqueeze(0), embedding_matrix[ii].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
