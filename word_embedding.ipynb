{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from datetime import datetime\n",
    "from utils import train, compute_cosine_sim\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "PATH_GENERATED = './generated/'\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith('.txt')]\n",
    "\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            texts += f.readlines()\n",
    "    return texts\n",
    "\n",
    "def tokenize(texts, tokenizer=TOKENIZER):\n",
    "    tokenized_text = []\n",
    "    for text in texts:\n",
    "        tokenized_text += tokenizer(text)\n",
    "    return tokenized_text\n",
    "\n",
    "def yield_tokens(texts, tokenizer=TOKENIZER):\n",
    "    \"\"\"\n",
    "    Remove yield tokens from the text before tokenizing\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove words with digits, upper case, and multiple space \n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    no_spaces = '\\s+'\n",
    "\n",
    "    for text in texts:\n",
    "        text = re.sub(no_digits, ' ', text)\n",
    "        text = re.sub(no_names, ' ', text)\n",
    "        text = re.sub(no_spaces, ' ', text)\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    vocab.append_token(\"i\")  # Upper case words like 'I' were removed so we should add it back again.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def calculate_word_weights(freqs):\n",
    "    \"\"\"\n",
    "    Calculate the weight of each word so that the loss function can weigh \n",
    "    frequent words less and unfrequent words more.\n",
    "    \"\"\"\n",
    "    total_words = sum(freqs)\n",
    "    word_weights = [total_words / (len(freqs)* freq) for freq in freqs]\n",
    "    word_weights = torch.tensor(word_weights, dtype=torch.float).to(device=device)\n",
    "    return word_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Tokenize texts -------------------------------\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\", map_location=torch.device(device))\n",
    "    words_val   = torch.load(PATH_GENERATED + \"words_val.pt\", map_location=torch.device(device))\n",
    "    words_test  = torch.load(PATH_GENERATED + \"words_test.pt\", map_location=torch.device(device))\n",
    "        \n",
    "else:\n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val   = read_files('./data_val/')\n",
    "    lines_books_test  = read_files('./data_test/')\n",
    "\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val   = tokenize(lines_books_val)\n",
    "    words_test  = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train, PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val, PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test, PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME, map_location=torch.device(device))\n",
    "else:\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "VOCAB_SIZE = len(vocab)\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "occurences = [(f.item(), w) for (f, w) in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))]\n",
    "word_weigts = calculate_word_weights(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      2684706\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   52105\n",
      "Number of distinct words kept (vocabulary size):    1880\n",
      "\n",
      "The 5 most frequent words:\n",
      " [(433907, '<unk>'), (182537, ','), (151278, 'the'), (123727, '.'), (82289, 'and')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "print(f\"\\nThe 5 most frequent words:\\n {occurences[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 5\n",
    "not_words = [',', '.', '(', ')', '?', '!', '<unk>']\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def create_dataset(text, vocab, context_size=CONTEXT_SIZE):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform each word to its index in the vocabulary.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        t = txt[i + context_size]\n",
    "        # We only want to guess actual words.\n",
    "        if vocab.lookup_token(t) in not_words: continue \n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(t) \n",
    "        contexts.append(torch.tensor(c).to(device=device))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets  = torch.tensor(targets).to(device=device)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname, map_location=torch.device(device))\n",
    "    else:\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val   = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test  = load_dataset(words_test, vocab, \"data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))        \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are testing 3 different hyper parameters.\n"
     ]
    }
   ],
   "source": [
    "embedding_dims = [10, 12, 16]\n",
    "\n",
    "hparams = [{\n",
    "    'embedding_dim': em\n",
    " } for em in embedding_dims]\n",
    "\n",
    "print(f\"We are testing {len(hparams)} different hyper parameters.\")\n",
    "\n",
    "train_loader = DataLoader(data_train, batch_size=128, shuffle=True)\n",
    "val_loader   = DataLoader(data_val, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(data_test, batch_size=128, shuffle=True)\n",
    "\n",
    "train_val_loader = DataLoader(ConcatDataset([data_train, data_val]), batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_models(hparams=hparams, n_epochs=5):\n",
    "\n",
    "    # ---------------- Train many models ----------------------- \n",
    "    models = []\n",
    "    train_sims = []\n",
    "    val_sims = []\n",
    "\n",
    "    for param in hparams:\n",
    "        print(f'Now training with parameters {param}')\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        embedding = nn.Embedding(VOCAB_SIZE, param['embedding_dim'])\n",
    "        torch.manual_seed(seed)\n",
    "        model = Word2Vec(embedding).to(device=device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "\n",
    "        train(n_epochs, optimizer, model, loss_fn, train_loader, device)\n",
    "\n",
    "        train_sim = compute_cosine_sim(model, train_loader, device)\n",
    "        print(f'Train similarity: {train_sim}')\n",
    "        val_sim = compute_cosine_sim(model, val_loader, device)\n",
    "        print(f'Val similarity:   {val_sim}')\n",
    "        \n",
    "        models.append(model)\n",
    "        train_sims.append(train_sim)\n",
    "        val_sims.append(val_sim)\n",
    "        print()\n",
    "    \n",
    "    return models, train_sims, val_sims\n",
    "\n",
    "def select_best_model(models, val_sims, hparams=hparams, n_epochs=25):\n",
    "\n",
    "    # ---------------- Select the best model ----------------------- \n",
    "    best_idx   = val_sims.index(max(val_sims))\n",
    "    best_model = models[best_idx]\n",
    "    best_param = hparams[best_idx]\n",
    "    print(f'The best model had these parameters: {best_param}.')\n",
    "\n",
    "    # ---------------- Retrain the best performing model for longer on more data ----------------------- \n",
    "    embedding = nn.Embedding(VOCAB_SIZE, best_param['embedding_dim'])\n",
    "    torch.manual_seed(seed)\n",
    "    best_model = Word2Vec(embedding).to(device=device)\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "    train(n_epochs, optimizer, best_model, loss_fn, train_val_loader, device)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity on test set: 0.53\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Best Model -------------------------------\n",
    "if os.path.isfile(PATH_GENERATED + 'best_model.pt'):\n",
    "    best_model = torch.load(PATH_GENERATED + 'best_model.pt', map_location=torch.device(device))\n",
    "else:\n",
    "    models, train_sims, val_sims = train_all_models()\n",
    "    best_model = select_best_model(models, val_sims)\n",
    "    torch.save(best_model, PATH_GENERATED + 'best_model.pt')\n",
    "\n",
    "torch.save(best_model.embedding, PATH_GENERATED + 'embedding.pt')\n",
    "\n",
    "# ----------------------- Evaluate Best Model -------------------------------\n",
    "test_sim = compute_cosine_sim(best_model, test_loader, device)\n",
    "print(f\"Similarity on test set: {round(float(test_sim), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Calculate Cosine Similarity Matrix -------------------------------\n",
    "cosineSimilarity = nn.CosineSimilarity(dim=2)\n",
    "embedding = best_model.embedding\n",
    "embedding_data = best_model.embedding.weight.data\n",
    "cos_matrix = cosineSimilarity(embedding_data.unsqueeze(0), embedding_data.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten randomly selected words with their most similar words:\n",
      "\n",
      "branch, details with similarity 0.8\n",
      "thrown, begins with similarity 0.75\n",
      "reach, filled with similarity 0.71\n",
      "provided, beauty with similarity 0.8\n",
      "week, sea with similarity 0.78\n",
      "becomes, are with similarity 0.75\n",
      "motionless, blue with similarity 0.79\n",
      "repeated, promise with similarity 0.82\n",
      "wounded, brother with similarity 0.76\n",
      "finally, already with similarity 0.88\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Randomly select 10 not too frequent words -------------------------------\n",
    "random.seed(seed)\n",
    "selected_words = random.sample([word for word in vocab.lookup_tokens(range(100,VOCAB_SIZE)) if word not in not_words], 10)\n",
    "selected_indecies = vocab.lookup_indices(selected_words)\n",
    "similar_words = []\n",
    "\n",
    "for idx in selected_indecies:\n",
    "    word_matrix = cos_matrix[idx].clone()\n",
    "    word_matrix[idx] = -1 # Every word is most like itself \n",
    "    similar_words.append((vocab.lookup_token(torch.argmax(word_matrix)), torch.max(word_matrix)))\n",
    "\n",
    "selected_similar_list = [(selected, similar, round(float(value), 2)) for (selected, (similar, value)) in zip(selected_words, similar_words)]\n",
    "\n",
    "print(\"Ten randomly selected words with their most similar words:\\n\")\n",
    "for w1, w2, sim in selected_similar_list:\n",
    "    print(f\"{w1}, {w2} with similarity {sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Convert embedding to tsv files -------------------------------\n",
    "with open(PATH_GENERATED + 'vocab.tsv', 'w', newline='') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
    "    for word in vocab.lookup_tokens(range(VOCAB_SIZE)):\n",
    "        writer.writerow([word])\n",
    "with open(PATH_GENERATED + 'embedding.tsv', 'w', newline='') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for word in embedding_data:\n",
    "        word = [float(w) for w in word]\n",
    "        writer.writerow(word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
