{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from datetime import datetime\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "PATH_GENERATED = './generated/'\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith('.txt')]\n",
    "\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            texts += f.readlines()\n",
    "    return texts\n",
    "\n",
    "def tokenize(texts, tokenizer=TOKENIZER):\n",
    "    tokenized_text = []\n",
    "    for text in texts:\n",
    "        tokenized_text += tokenizer(text)\n",
    "    return tokenized_text\n",
    "\n",
    "def yield_tokens(texts, tokenizer=TOKENIZER):\n",
    "    \"\"\"\n",
    "    Remove yield tokens from the text before tokenizing\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove words with digits, upper case, and multiple space \n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    no_spaces = '\\s+'\n",
    "\n",
    "    for text in texts:\n",
    "        text = re.sub(no_digits, ' ', text)\n",
    "        text = re.sub(no_names, ' ', text)\n",
    "        text = re.sub(no_spaces, ' ', text)\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    vocab.append_token(\"i\")  # Upper case words like 'I' were removed so we should add it back again.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def calculate_word_weights(freqs):\n",
    "    \"\"\"\n",
    "    Calculate the weight of each word so that the loss function can weigh \n",
    "    frequent words less and unfrequent words more.\n",
    "    \"\"\"\n",
    "    total_words = sum(freqs)\n",
    "    word_weights = [total_words / (len(freqs)* freq) for freq in freqs]\n",
    "    word_weights = torch.tensor(word_weights, dtype=torch.float).to(device=device)\n",
    "    return word_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Tokenize texts -------------------------------\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\", map_location=torch.device(device))\n",
    "    words_val   = torch.load(PATH_GENERATED + \"words_val.pt\", map_location=torch.device(device))\n",
    "    words_test  = torch.load(PATH_GENERATED + \"words_test.pt\", map_location=torch.device(device))\n",
    "        \n",
    "else:\n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val   = read_files('./data_val/')\n",
    "    lines_books_test  = read_files('./data_test/')\n",
    "\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val   = tokenize(lines_books_val)\n",
    "    words_test  = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train, PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val, PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test, PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME, map_location=torch.device(device))\n",
    "else:\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "occurences = [(f.item(), w) for (f, w) in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))]\n",
    "word_weigts = calculate_word_weights(freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      2684706\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   52105\n",
      "Number of distinct words kept (vocabulary size):    1880\n",
      "The 10 most occuring words:\n",
      " [(433907, '<unk>'), (182537, ','), (151278, 'the'), (123727, '.'), (82289, 'and'), (65661, 'of'), (62763, 'to'), (49230, 'a'), (41477, 'in'), (31052, 'that')]\n"
     ]
    }
   ],
   "source": [
    "n_print = 10\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "print(f\"The {n_print} most occuring words:\\n {occurences[:n_print]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "# true labels for this task:\n",
    "MAP_TARGET = {vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))}\n",
    "\n",
    "def create_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=MAP_TARGET):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform each word to its index in the vocabulary.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        t = txt[i + context_size]\n",
    "        if map_target[t] < 2: continue # We only want to guess actual words.\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(t) \n",
    "        contexts.append(torch.tensor(c).to(device=device))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets).to(device=device)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname, map_location=torch.device(device))\n",
    "    else:\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val   = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test  = load_dataset(words_test, vocab, \"data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))        \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader, device=None):\n",
    "    model.eval()\n",
    "    embedding_dim = model.embedding.weight.data.shape[1]\n",
    "\n",
    "    dist = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += len(targets)\n",
    "            dist += torch.dist(predicted.float(), targets.float()) / embedding_dim\n",
    "\n",
    "    acc =  dist / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are testing 6 different hyper parameters.\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "embedding_dims = [10, 16]\n",
    "\n",
    "hparams = [{\n",
    "    'batch_size': bs,\n",
    "    'embedding_dim': em\n",
    " } for bs in batch_sizes for em in embedding_dims]\n",
    "\n",
    "print(f\"We are testing {len(hparams)} different hyper parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model():\n",
    "\n",
    "    # ---------------- Train many models ----------------------- \n",
    "    models = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for param in hparams:\n",
    "        print(f'Now training with parameters {param}')\n",
    "        train_loader = DataLoader(data_train, batch_size=param['batch_size'], shuffle=True)\n",
    "        val_loader   = DataLoader(data_val, batch_size=param['batch_size'], shuffle=True)\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        embedding = nn.Embedding(VOCAB_SIZE, param['embedding_dim'])\n",
    "        torch.manual_seed(seed)\n",
    "        model = Word2Vec(embedding).to(device=device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "        n_epochs=5\n",
    "        train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "\n",
    "        train_acc = compute_accuracy(model, train_loader, device)\n",
    "        print(f'Train accuracy: {train_acc}')\n",
    "        val_acc = compute_accuracy(model, val_loader, device)\n",
    "        print(f'Val accuracy:   {val_acc}')\n",
    "        \n",
    "        models.append(model)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        print()\n",
    "\n",
    "    # ---------------- Retrain the best performing model for longer on more data -----------------------   \n",
    "    best_idx = val_accs.index(min(val_accs))\n",
    "    best_model = models[best_idx]\n",
    "    best_param = hparams[best_idx]\n",
    "    \n",
    "    print(f'The best model had these parameters: {best_param}.')\n",
    "    train_val_loader = DataLoader(ConcatDataset([data_train, data_val]), batch_size=best_param['batch_size'], shuffle=True)\n",
    "    embedding = nn.Embedding(VOCAB_SIZE, best_param['embedding_dim'])\n",
    "    torch.manual_seed(seed)\n",
    "    best_model = Word2Vec(embedding).to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "    n_epochs=50\n",
    "    losses_train = train(n_epochs, optimizer, best_model, loss_fn, train_val_loader)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Best Model -------------------------------\n",
    "if os.path.isfile(PATH_GENERATED + 'best_model.pt'):\n",
    "    best_model = torch.load(PATH_GENERATED + 'best_model.pt', map_location=torch.device(device))\n",
    "else:\n",
    "    best_model = train_best_model()\n",
    "    torch.save(best_model, PATH_GENERATED + 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Calculate Cosine Similarity Matrix -------------------------------\n",
    "cosineSimilarity = nn.CosineSimilarity(dim=2)\n",
    "embedding = best_model.embedding.weight.data\n",
    "cos_matrix = cosineSimilarity(embedding.unsqueeze(0), embedding.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('branch', 'proceed', 0.84),\n",
       " ('thrown', 'heap', 0.88),\n",
       " ('reach', 'towards', 0.76),\n",
       " ('provided', 'allowed', 0.75),\n",
       " ('week', 'play', 0.69),\n",
       " ('becomes', 'becoming', 0.76),\n",
       " ('motionless', 'erect', 0.77),\n",
       " ('repeated', 'hearing', 0.72),\n",
       " ('wounded', 'forgotten', 0.75),\n",
       " ('finally', 'abruptly', 0.79)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------- Randomly select 10 words from 1000 most frequent -------------------------------\n",
    "random.seed(seed)\n",
    "selected_words = random.sample([word for word in vocab.lookup_tokens(range(100,VOCAB_SIZE)) if MAP_TARGET[vocab[word]] == 2], 10)\n",
    "selected_indecies = vocab.lookup_indices(selected_words)\n",
    "similar_words = []\n",
    "\n",
    "for idx in selected_indecies:\n",
    "    word_matrix = cos_matrix[idx].clone()\n",
    "    word_matrix[idx] = -1 # Every word is most like itself \n",
    "    similar_words.append((vocab.lookup_token(torch.argmax(word_matrix)), torch.max(word_matrix)))\n",
    "\n",
    "selected_similar_list = [(selected, similar, round(float(value), 2)) for (selected, (similar, value)) in zip(selected_words, similar_words)]\n",
    "selected_similar_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
