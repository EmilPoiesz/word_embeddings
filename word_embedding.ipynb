{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import random\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, ConcatDataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from datetime import datetime\n",
    "\n",
    "seed = 265\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer('basic_english')\n",
    "PATH_GENERATED = './generated/'\n",
    "MIN_FREQ = 100\n",
    "\n",
    "def read_files(datapath='./data_train/'):\n",
    "    files = os.listdir(datapath)\n",
    "    files = [datapath + f for f in files if f.endswith('.txt')]\n",
    "\n",
    "    texts = []\n",
    "    for file in files:\n",
    "        with open(file) as f:\n",
    "            texts += f.readlines()\n",
    "    return texts\n",
    "\n",
    "def tokenize(texts, tokenizer=TOKENIZER):\n",
    "    tokenized_text = []\n",
    "    for text in texts:\n",
    "        tokenized_text += tokenizer(text)\n",
    "    return tokenized_text\n",
    "\n",
    "def yield_tokens(texts, tokenizer=TOKENIZER):\n",
    "    \"\"\"\n",
    "    Remove yield tokens from the text before tokenizing\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove words with digits, upper case, and multiple space \n",
    "    no_digits = '\\w*[0-9]+\\w*'\n",
    "    no_names = '\\w*[A-Z]+\\w*'\n",
    "    no_spaces = '\\s+'\n",
    "\n",
    "    for text in texts:\n",
    "        text = re.sub(no_digits, ' ', text)\n",
    "        text = re.sub(no_names, ' ', text)\n",
    "        text = re.sub(no_spaces, ' ', text)\n",
    "        yield tokenizer(text)\n",
    "\n",
    "def count_freqs(words, vocab):\n",
    "    freqs = torch.zeros(len(vocab), dtype=torch.int)\n",
    "    for w in words:\n",
    "        freqs[vocab[w]] += 1\n",
    "    return freqs\n",
    "\n",
    "def create_vocabulary(lines, min_freq=MIN_FREQ):\n",
    "    \"\"\"\n",
    "    Create a vocabulary (list of known tokens) from a list of strings\n",
    "    \"\"\"\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(lines), min_freq=min_freq, specials=[\"<unk>\"])\n",
    "    vocab.append_token(\"i\")  # Upper case words like 'I' were removed so we should add it back again.\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def calculate_word_weights(freqs):\n",
    "    \"\"\"\n",
    "    Calculate the weight of each word so that the loss function can weigh \n",
    "    frequent words less and unfrequent words more.\n",
    "    \"\"\"\n",
    "    total_words = sum(freqs)\n",
    "    word_weights = [total_words / (len(freqs)* freq) for freq in freqs]\n",
    "    word_weights = torch.tensor(word_weights, dtype=torch.float).to(device=device)\n",
    "    return word_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Tokenize texts -------------------------------\n",
    "\n",
    "if os.path.isfile(PATH_GENERATED + \"words_train.pt\"):\n",
    "    words_train = torch.load(PATH_GENERATED + \"words_train.pt\", map_location=torch.device(device))\n",
    "    words_val   = torch.load(PATH_GENERATED + \"words_val.pt\", map_location=torch.device(device))\n",
    "    words_test  = torch.load(PATH_GENERATED + \"words_test.pt\", map_location=torch.device(device))\n",
    "        \n",
    "else:\n",
    "    lines_books_train = read_files('./data_train/')\n",
    "    lines_books_val   = read_files('./data_val/')\n",
    "    lines_books_test  = read_files('./data_test/')\n",
    "\n",
    "    words_train = tokenize(lines_books_train)\n",
    "    words_val   = tokenize(lines_books_val)\n",
    "    words_test  = tokenize(lines_books_test)\n",
    "    \n",
    "    torch.save(words_train, PATH_GENERATED + \"words_train.pt\")\n",
    "    torch.save(words_val, PATH_GENERATED + \"words_val.pt\")\n",
    "    torch.save(words_test, PATH_GENERATED + \"words_test.pt\")\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------- Create vocabulary ----------------------------\n",
    "\n",
    "VOCAB_FNAME = \"vocabulary.pt\"\n",
    "if os.path.isfile(PATH_GENERATED + VOCAB_FNAME):\n",
    "    vocab = torch.load(PATH_GENERATED + VOCAB_FNAME, map_location=torch.device(device))\n",
    "else:\n",
    "    vocab = create_vocabulary(lines_books_train, min_freq=MIN_FREQ)\n",
    "    torch.save(vocab, PATH_GENERATED + VOCAB_FNAME)\n",
    "    \n",
    "\n",
    "\n",
    "# ------------------------ Quick analysis ------------------------------\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "freqs = count_freqs(words_train, vocab)\n",
    "occurences = [(f.item(), w) for (f, w) in zip(freqs, vocab.lookup_tokens(range(VOCAB_SIZE)))]\n",
    "word_weigts = calculate_word_weights(freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in the training dataset:      2684706\n",
      "Total number of words in the validation dataset:    49526\n",
      "Total number of words in the test dataset:          124152\n",
      "Number of distinct words in the training dataset:   52105\n",
      "Number of distinct words kept (vocabulary size):    1880\n",
      "The 10 most occuring words:\n",
      " [(433907, '<unk>'), (182537, ','), (151278, 'the'), (123727, '.'), (82289, 'and'), (65661, 'of'), (62763, 'to'), (49230, 'a'), (41477, 'in'), (31052, 'that')]\n"
     ]
    }
   ],
   "source": [
    "n_print = 10\n",
    "print(\"Total number of words in the training dataset:     \", len(words_train))\n",
    "print(\"Total number of words in the validation dataset:   \", len(words_val))\n",
    "print(\"Total number of words in the test dataset:         \", len(words_test))\n",
    "print(\"Number of distinct words in the training dataset:  \", len(set(words_train)))\n",
    "print(\"Number of distinct words kept (vocabulary size):   \", VOCAB_SIZE)\n",
    "\n",
    "print(f\"The {n_print} most occuring words:\\n {occurences[:n_print]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_SIZE = 3\n",
    "\n",
    "# ---------------- Define context / target pairs -----------------------\n",
    "def compute_label(w):\n",
    "    \"\"\"\n",
    "    helper function to define MAP_TARGET\n",
    "    \n",
    "    - 0 = 'unknown word'\n",
    "    - 1 = 'punctuation' (i.e. the '<unk>' token)\n",
    "    - 2 = 'is an actual word'\n",
    "    \"\"\"\n",
    "    if w in ['<unk>']:\n",
    "        return 0\n",
    "    elif w in [',', '.', '(', ')', '?', '!']:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "MAP_TARGET = {vocab[w]:compute_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))}\n",
    "\n",
    "def create_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=MAP_TARGET):\n",
    "    \"\"\"\n",
    "    Create a pytorch dataset of context / target pairs from a text\n",
    "    \"\"\"\n",
    "    \n",
    "    # Transform each word to its index in the vocabulary.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        t = txt[i + context_size]\n",
    "        if map_target[t] < 2: continue # We only want to guess actual words.\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(t) \n",
    "        contexts.append(torch.tensor(c).to(device=device))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets).to(device=device)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname, map_location=torch.device(device))\n",
    "    else:\n",
    "        dataset = create_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train = load_dataset(words_train, vocab, \"data_train.pt\")\n",
    "data_val   = load_dataset(words_val, vocab, \"data_val.pt\")\n",
    "data_test  = load_dataset(words_test, vocab, \"data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.embedding = embedding\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 128)\n",
    "        self.fc2 = nn.Linear(128, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))        \n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, optimizer, model, loss_fn, train_loader, device=None):\n",
    "\n",
    "    n_batch = len(train_loader)\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "        loss_train = 0.0\n",
    "        for contexts, targets in train_loader:\n",
    "\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        losses_train.append(loss_train / n_batch)\n",
    "\n",
    "        if epoch == 1 or epoch % 5 == 0:\n",
    "            print('{}  |  Epoch {}  |  Training loss {:.5f}'.format(\n",
    "                datetime.now().time(), epoch, loss_train / n_batch))\n",
    "    return losses_train\n",
    "\n",
    "\n",
    "def compute_accuracy(model, loader, device=None):\n",
    "    model.eval()\n",
    "    embedding_dim = model.embedding.weight.data.shape[1]\n",
    "\n",
    "    dist = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for contexts, targets in loader:\n",
    "            contexts = contexts.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "\n",
    "            outputs = model(contexts)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += len(targets)\n",
    "            dist += torch.dist(predicted.float(), targets.float()) / embedding_dim\n",
    "\n",
    "    acc =  dist / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are testing 6 different hyper parameters.\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [64, 128, 256]\n",
    "embedding_dims = [10, 16]\n",
    "\n",
    "hparams = [{\n",
    "    'batch_size': bs,\n",
    "    'embedding_dim': em\n",
    " } for bs in batch_sizes for em in embedding_dims]\n",
    "\n",
    "print(f\"We are testing {len(hparams)} different hyper parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_best_model():\n",
    "\n",
    "    # ---------------- Train many models ----------------------- \n",
    "    models = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "\n",
    "    for param in hparams:\n",
    "        print(f'Now training with parameters {param}')\n",
    "        train_loader = DataLoader(data_train, batch_size=param['batch_size'], shuffle=True)\n",
    "        val_loader   = DataLoader(data_val, batch_size=param['batch_size'], shuffle=True)\n",
    "\n",
    "        torch.manual_seed(seed)\n",
    "        embedding = nn.Embedding(VOCAB_SIZE, param['embedding_dim'])\n",
    "        torch.manual_seed(seed)\n",
    "        model = Word2Vec(embedding).to(device=device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "        n_epochs=5\n",
    "        train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "\n",
    "        train_acc = compute_accuracy(model, train_loader, device)\n",
    "        print(f'Train accuracy: {train_acc}')\n",
    "        val_acc = compute_accuracy(model, val_loader, device)\n",
    "        print(f'Val accuracy:   {val_acc}')\n",
    "        \n",
    "        models.append(model)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        print()\n",
    "\n",
    "    # ---------------- Retrain the best performing model for longer on more data -----------------------   \n",
    "    best_idx = val_accs.index(min(val_accs))\n",
    "    best_model = models[best_idx]\n",
    "    best_param = hparams[best_idx]\n",
    "    \n",
    "    print(f'The best model had these parameters: {best_param}.')\n",
    "    train_val_loader = DataLoader(ConcatDataset([data_train, data_val]), batch_size=best_param['batch_size'], shuffle=True)\n",
    "    embedding = nn.Embedding(VOCAB_SIZE, best_param['embedding_dim'])\n",
    "    torch.manual_seed(seed)\n",
    "    best_model = Word2Vec(embedding).to(device=device)\n",
    "\n",
    "    optimizer = optim.Adam(best_model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=word_weigts)\n",
    "    n_epochs=50\n",
    "    losses_train = train(n_epochs, optimizer, best_model, loss_fn, train_val_loader)\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59.4573)\n",
      "tensor(60.5469)\n",
      "tensor(60.7820)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------- Best Model -------------------------------\n",
    "if os.path.isfile(PATH_GENERATED + 'best_model.pt'):\n",
    "    best_model = torch.load(PATH_GENERATED + 'best_model.pt', map_location=torch.device(device))\n",
    "else:\n",
    "    best_model = train_best_model()\n",
    "    torch.save(best_model, PATH_GENERATED + 'best_model.pt')\n",
    "\n",
    "# ----------------------- Evaluate Best Model -------------------------------\n",
    "test_loader = DataLoader(data_test, shuffle=True)\n",
    "test_acc = compute_accuracy(best_model, test_loader)\n",
    "print(test_acc)\n",
    "print(compute_accuracy(best_model, DataLoader(data_train, shuffle=True)))\n",
    "print(compute_accuracy(best_model, DataLoader(data_val, shuffle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Calculate Cosine Similarity Matrix -------------------------------\n",
    "cosineSimilarity = nn.CosineSimilarity(dim=2)\n",
    "embedding = best_model.embedding\n",
    "embedding_data = best_model.embedding.weight.data\n",
    "cos_matrix = cosineSimilarity(embedding_data.unsqueeze(0), embedding_data.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('branch', 'proceed', 0.84),\n",
       " ('thrown', 'heap', 0.88),\n",
       " ('reach', 'towards', 0.76),\n",
       " ('provided', 'allowed', 0.75),\n",
       " ('week', 'play', 0.69),\n",
       " ('becomes', 'becoming', 0.76),\n",
       " ('motionless', 'erect', 0.77),\n",
       " ('repeated', 'hearing', 0.72),\n",
       " ('wounded', 'forgotten', 0.75),\n",
       " ('finally', 'abruptly', 0.79)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------- Randomly select 10 words from 1000 most frequent -------------------------------\n",
    "random.seed(seed)\n",
    "selected_words = random.sample([word for word in vocab.lookup_tokens(range(100,VOCAB_SIZE)) if MAP_TARGET[vocab[word]] == 2], 10)\n",
    "selected_indecies = vocab.lookup_indices(selected_words)\n",
    "similar_words = []\n",
    "\n",
    "for idx in selected_indecies:\n",
    "    word_matrix = cos_matrix[idx].clone()\n",
    "    word_matrix[idx] = -1 # Every word is most like itself \n",
    "    similar_words.append((vocab.lookup_token(torch.argmax(word_matrix)), torch.max(word_matrix)))\n",
    "\n",
    "selected_similar_list = [(selected, similar, round(float(value), 2)) for (selected, (similar, value)) in zip(selected_words, similar_words)]\n",
    "selected_similar_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------- Convert embedding to tsv files -------------------------------\n",
    "with open(PATH_GENERATED + 'vocab.tsv', 'w', newline='') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
    "    for word in vocab.lookup_tokens(range(VOCAB_SIZE)):\n",
    "        writer.writerow([word])\n",
    "with open(PATH_GENERATED + 'embedding.tsv', 'w', newline='') as tsvfile:\n",
    "    writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "    for word in embedding_data:\n",
    "        word = [float(w) for w in word]\n",
    "        writer.writerow(word)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conjunction of _be_ and _have_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_list = ['be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "\n",
    "def compute_conjunction_label(w):\n",
    "    return w in ['be', 'am', 'are', 'is', 'was', 'were', 'been', 'being', 'have', 'has', 'had', 'having']\n",
    "\n",
    "CONJ_TARGET = {vocab[w]:compute_conjunction_label(w) for w in vocab.lookup_tokens(range(VOCAB_SIZE))}\n",
    "\n",
    "def create_conjunction_dataset(text, vocab, context_size=CONTEXT_SIZE, map_target=CONJ_TARGET):\n",
    "    \n",
    "    # Transform each word to its index in the vocabulary.\n",
    "    txt = [vocab[w] for w in text]\n",
    "\n",
    "    n_text = len(text)\n",
    "    contexts = []\n",
    "    targets = []\n",
    "    for i in range(n_text - context_size):\n",
    "        \n",
    "        t = txt[i + context_size]\n",
    "        if map_target[t] == False: continue # We only want to guess conjunctions of be and have.\n",
    "        c = txt[i:i + context_size]\n",
    "        \n",
    "        targets.append(t) \n",
    "        contexts.append(torch.tensor(c).to(device=device))\n",
    "            \n",
    "    contexts = torch.stack(contexts)\n",
    "    targets = torch.tensor(targets).to(device=device)\n",
    "    return TensorDataset(contexts, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conjunction_dataset(words, vocab, fname):\n",
    "    \"\"\"\n",
    "    Load dataset if already generated, otherwise, create it and save it\n",
    "    \"\"\"\n",
    "    \n",
    "    if os.path.isfile(PATH_GENERATED + fname):\n",
    "        dataset = torch.load(PATH_GENERATED + fname, map_location=torch.device(device))\n",
    "    else:\n",
    "        dataset = create_conjunction_dataset(words, vocab)\n",
    "        torch.save(dataset, PATH_GENERATED + fname)\n",
    "    return dataset\n",
    "\n",
    "data_train_conj = load_conjunction_dataset(words_train, vocab, \"conj_data_train.pt\")\n",
    "data_val_conj   = load_conjunction_dataset(words_val, vocab, \"conj_data_val.pt\")\n",
    "data_test_conj  = load_conjunction_dataset(words_test, vocab, \"conj_data_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, embedding, context_size=CONTEXT_SIZE):\n",
    "        super().__init__()\n",
    "\n",
    "        (vocab_size, embedding_dim) = embedding.weight.shape\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.load_state_dict(embedding.state_dict())\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(embedding_dim*context_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = F.relu(self.fc1(torch.flatten(out, 1)))\n",
    "        out = F.relu(self.fc2(out, 1))\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- Simple MLP hyper parameters -----------------------\n",
    "lrs = [0.01, 0.001]\n",
    "decays = [0.8, 0.5, 0.1]\n",
    "mlp_hparams = [{\n",
    "    'lr': lr,\n",
    "    'weight_decay': decay\n",
    "} for lr in lrs for decay in decays]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Linear.forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/emil/Documents/inf265/Projects/Project 3/word_embedding.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m val_loader \u001b[39m=\u001b[39m DataLoader(data_val_conj, batch_size\u001b[39m=\u001b[39m\u001b[39m128\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m n_epochs \u001b[39m=\u001b[39m \u001b[39m5\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m loss \u001b[39m=\u001b[39m train(n_epochs, optimizer, model, loss_fn, train_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m accuracy \u001b[39m=\u001b[39m compute_accuracy(model, val_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m models\u001b[39m.\u001b[39mappend(model)\n",
      "\u001b[1;32m/Users/emil/Documents/inf265/Projects/Project 3/word_embedding.ipynb Cell 20\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m contexts \u001b[39m=\u001b[39m contexts\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(contexts)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(outputs, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/inf265/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/emil/Documents/inf265/Projects/Project 3/word_embedding.ipynb Cell 20\u001b[0m in \u001b[0;36mSimpleMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc1(torch\u001b[39m.\u001b[39mflatten(out, \u001b[39m1\u001b[39m)))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc2(out, \u001b[39m1\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/emil/Documents/inf265/Projects/Project%203/word_embedding.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/inf265/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Linear.forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "train_losses = []\n",
    "models = []\n",
    "accuracies = []\n",
    "\n",
    "for param in mlp_hparams:\n",
    "    model = SimpleMLP(embedding)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), **param)\n",
    "    train_loader = DataLoader(data_test_conj, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(data_val_conj, batch_size=128, shuffle=True)\n",
    "    n_epochs = 5\n",
    "    \n",
    "    loss = train(n_epochs, optimizer, model, loss_fn, train_loader)\n",
    "    accuracy = compute_accuracy(model, val_loader)\n",
    "\n",
    "    models.append(model)\n",
    "    train_losses.append(loss)\n",
    "    accuracies.append(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
